{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 14, "column": 0}, "map": {"version":3,"sources":["file:///Users/abhijeetherra/Documents/portfolio/components/SectionHeader.tsx"],"sourcesContent":["interface SectionHeaderProps {\n  title: string\n  subtitle?: string\n}\n\nexport default function SectionHeader({ title, subtitle }: SectionHeaderProps) {\n  return (\n    <header className=\"mb-10\">\n      <h2 className=\"font-serif text-3xl font-bold tracking-tight text-gray-900 md:text-4xl\">\n        {title}\n      </h2>\n      {subtitle && (\n        <p className=\"mt-2 max-w-2xl font-sans text-lg text-gray-600\">{subtitle}</p>\n      )}\n    </header>\n  )\n}\n"],"names":[],"mappings":";;;;;;AAKe,SAAS,cAAc,EAAE,KAAK,EAAE,QAAQ,EAAsB;IAC3E,qBACE,8OAAC;QAAO,WAAU;;0BAChB,8OAAC;gBAAG,WAAU;0BACX;;;;;;YAEF,0BACC,8OAAC;gBAAE,WAAU;0BAAkD;;;;;;;;;;;;AAIvE"}},
    {"offset": {"line": 51, "column": 0}, "map": {"version":3,"sources":["file:///Users/abhijeetherra/Documents/portfolio/components/TagBadge.tsx"],"sourcesContent":["export default function TagBadge({ label }: { label: string }) {\n  return (\n    <span className=\"inline-flex items-center rounded-md bg-accent/10 px-2.5 py-0.5 text-xs font-medium text-accent transition-colors hover:bg-accent/15\">\n      {label}\n    </span>\n  )\n}\n"],"names":[],"mappings":";;;;;;AAAe,SAAS,SAAS,EAAE,KAAK,EAAqB;IAC3D,qBACE,8OAAC;QAAK,WAAU;kBACb;;;;;;AAGP"}},
    {"offset": {"line": 71, "column": 0}, "map": {"version":3,"sources":["file:///Users/abhijeetherra/Documents/portfolio/app/projects/abra/page.tsx"],"sourcesContent":["import type { Metadata } from 'next'\nimport Link from 'next/link'\nimport SectionHeader from '@/components/SectionHeader'\nimport TagBadge from '@/components/TagBadge'\n\nexport const metadata: Metadata = {\n  title: 'ABRA — Auditory Brainstem Response Analyzer',\n  description:\n    'ABR analysis pipeline with preprocessing and CNN models for automated peak finding and threshold detection. Streamlit app, .arf, .tsv, .asc, .csv.',\n}\n\nexport default function ABRAPage() {\n  return (\n    <article className=\"mx-auto max-w-3xl px-6 py-12 md:py-16\">\n      <Link\n        href=\"/\"\n        className=\"mb-8 inline-block font-sans text-sm text-gray-500 transition-colors hover:text-accent\"\n      >\n        ← Back to home\n      </Link>\n\n      <header className=\"mb-12\">\n        <div className=\"mb-4 flex flex-wrap gap-2\">\n          <TagBadge label=\"Python\" />\n          <TagBadge label=\"PyTorch\" />\n          <TagBadge label=\"CNN\" />\n          <TagBadge label=\"Neuroscience\" />\n        </div>\n        <h1 className=\"font-serif text-4xl font-bold tracking-tight text-gray-900 md:text-5xl\">\n          ABRA\n        </h1>\n        <p className=\"mt-2 font-sans text-lg text-gray-600\">\n          Auditory Brainstem Response Analyzer\n        </p>\n      </header>\n\n      <section className=\"mb-16\">\n        <p className=\"font-sans text-gray-700 leading-relaxed\">\n          ABRA is an Auditory Brainstem Response (ABR) analysis pipeline whose core\n          is a pair of convolutional neural networks built on top of a\n          preprocessing stage: a <strong>peak-finding CNN</strong> that localizes\n          the Wave I peak, and a <strong>thresholding CNN</strong> that predicts \n          the signal detection threshold from waveform-stack data.\n          Raw ABR data (Tucker Davis .arf, EPFL .tsv/.asc, or standardized .csv) is\n          normalized and cleaned first; the preprocessed waveforms then feed into\n          these models to automate what has traditionally required manual marking and\n          subjective threshold calls. Users can run this pipeline via a Streamlit web app\n          for batch upload and analysis, with optional manual override. The project\n          is developed with the UCSD Manor Lab; method and validation are described\n          in the bioRxiv preprint.\n        </p>\n      </section>\n\n      <hr className=\"my-12 border-gray-200\" />\n\n      <section className=\"mb-16\">\n        <SectionHeader title=\"From preprocessing to CNNs\" />\n        <p className=\"mb-4 font-sans text-gray-700 leading-relaxed\">\n          Preprocessing takes raw files and\n          produces a unified representation of ABR waveforms: format-specific\n          parsing (e.g. attenuation vs absolute dB SPL for .arf), alignment of\n          level and frequency dimensions, and optional filtering or baseline\n          correction. The result is a vector representation of each waveform\n          that the CNNs expect for peak-finding and thresholding.\n        </p>\n        <p className=\"mb-4 font-sans text-gray-700 leading-relaxed\">\n          The <strong>peak-finding CNN</strong> operates on these preprocessed\n          waveforms. It is a convolutional network (1D convolutions along the time\n          axis) trained to regress peak\n          locations for wave I of the ABR.\n          The model learns to be robust to\n          noise and level-dependent morphology so that peak tables can be generated\n          automatically across conditions.\n        </p>\n        <p className=\"font-sans text-gray-700 leading-relaxed\">\n          The <strong>thresholding CNN</strong> takes preprocessed\n          waveform-stack data and predicts the signal detection threshold—the stimulus level\n          (e.g. dB SPL) at which the ABR is deemed present. This is done viaclassification (\n          present/absent per level), with the CNN capturing the relationship\n          between waveform shape and detectability. Together, the peak and\n          threshold models turn raw preprocessed ABR data into structured outputs\n          (peak latencies/amplitudes, threshold estimates) that the Streamlit app\n          surfaces for users to review and export.\n        </p>\n      </section>\n\n      <hr className=\"my-12 border-gray-200\" />\n\n      <section className=\"mb-16\">\n        <SectionHeader title=\"Training & evaluation\" />\n        <p className=\"mb-4 font-sans text-gray-700 leading-relaxed\">\n          The convolutional neural networks are trained on\n          <strong> diverse datasets collected from multiple experimental settings</strong>.\n          This multi-site, multi-protocol training data is intended to improve\n          generalization so that the peak-finding and thresholding models perform well\n          across different laboratories, acquisition systems, and stimulus\n          parameters. Before training, each ABR is put through a data augmentation pipeline\n          including random noise addition, random amplitude scaling, and random time shift,\n          allowing the model to learn to be robust to and generalize for new data with these variations.\n          Preprocessed waveform-stack data are used as inputs; labels are expert manual\n          annotations (peak latencies and amplitudes, threshold judgments) so that\n          the models learn to replicate expert-level decisions in a consistent,\n          automated way. The training data is split into training, validation, and test sets to evaluate the model's performance.\n        </p>\n        <p className=\"mb-4 font-sans text-gray-700 leading-relaxed\">\n          <strong>Evaluation</strong> demonstrates that ABRA&rsquo;s\n          deep learning models achieve <strong>performance comparable to expert\n          human annotators</strong> on the key ABR metrics: peak amplitude, peak\n          latency, and auditory threshold estimates. In addition, the toolbox\n          <strong> dramatically reduces analysis time</strong> compared to manual\n          marking and threshold calls, and it <strong>enhances reproducibility</strong>{' '}\n          across datasets from different laboratories—addressing the variability\n          and limited reproducibility that traditionally accompany subjective\n          manual ABR interpretation. The app is used in the lab for batch analysis:\n          preprocessing runs first, then the CNNs produce peak tables and threshold\n          estimates that users can review and edit before export.\n        </p>\n      </section>\n\n      <hr className=\"my-12 border-gray-200\" />\n\n      <section className=\"mb-16\">\n        <SectionHeader title=\"Technical depth\" />\n        <p className=\"mb-4 font-sans text-gray-700 leading-relaxed\">\n          Preprocessing supports Tucker Davis .arf (with optional attenuation→dB SPL\n          handling), EPFL .tsv and .asc, and .csv with <code className=\"rounded bg-gray-100 px-1 py-0.5 text-sm\">Level(dB)</code> and{' '}\n          <code className=\"rounded bg-gray-100 px-1 py-0.5 text-sm\">Freq(Hz)</code> and a data vector per row. The\n          pipeline normalizes sampling, alignment, and units so that the\n          peak-finding and thresholding CNNs receive fixed-size or padded\n          inputs. Model artifacts (saved weights, config) are loaded at\n          inference; the Streamlit app (<code className=\"rounded bg-gray-100 px-1 py-0.5 text-sm\">ABRA.py</code>) calls\n          into the preprocessing and model code, then renders tables and plots.\n        </p>\n        <p className=\"font-sans text-gray-700 leading-relaxed\">\n          The codebase is organized so that preprocessing, peak model, and\n          threshold model are distinct modules; <code className=\"rounded bg-gray-100 px-1 py-0.5 text-sm\">utils</code> hold shared\n          logic, and <code className=\"rounded bg-gray-100 px-1 py-0.5 text-sm\">notebooks</code> demonstrate usage. An API-only\n          install (<code className=\"rounded bg-gray-100 px-1 py-0.5 text-sm\">requirements-api.txt</code>) allows\n          programmatic use of the preprocessing and CNN inference without the web\n          UI. Local runs use <code className=\"rounded bg-gray-100 px-1 py-0.5 text-sm\">streamlit run ABRA.py</code> in a\n          conda environment (Python 3.12).\n        </p>\n      </section>\n\n      <hr className=\"my-12 border-gray-200\" />\n\n      <section className=\"mb-16\">\n        <SectionHeader title=\"Tradeoffs\" />\n        <p className=\"mb-4 font-sans text-gray-700 leading-relaxed\">\n          Using CNNs for peak finding and thresholding trades interpretability for\n          automation and consistency: the models learn from data rather than\n          hand-tuned rules, but individual predictions are not easily explained.\n          Preprocessing is kept explicit and format-aware so that the CNNs see a\n          consistent input distribution; domain shifts (e.g. new acquisition\n          systems or protocols) may require retraining or fine-tuning. The\n          Streamlit UI and manual-edit workflows provide a safety net—researchers\n          can override model outputs and export corrected results—so the system\n          remains suitable for high-throughput batch analysis with human-in-the-loop\n          quality control.\n        </p>\n      </section>\n\n      <hr className=\"my-12 border-gray-200\" />\n\n      <section className=\"mb-16\">\n        <SectionHeader title=\"Limitations & future work\" />\n        <p className=\"mb-4 font-sans text-gray-700 leading-relaxed\">\n          The CNNs are trained on data from supported formats and acquisition\n          paradigms; performance may degrade on very different hardware or\n          protocols without retraining. Future work may include shared or\n          multi-task backbones for peak and threshold models, uncertainty\n          estimates (e.g. confidence intervals for threshold), and integration\n          with additional export and lab pipeline tools. The open-source repo\n          documents installation, usage, and extension points for the\n          preprocessing and CNN inference pipeline.\n        </p>\n      </section>\n\n      <footer className=\"pt-8 flex flex-wrap items-center gap-4\">\n        <a\n          href=\"https://github.com/ucsdmanorlab/abranalysis\"\n          target=\"_blank\"\n          rel=\"noopener noreferrer\"\n          className=\"font-sans text-sm text-accent transition-colors hover:underline\"\n        >\n          View on GitHub →\n        </a>\n        <a\n          href=\"https://www.biorxiv.org/content/10.1101/2024.06.20.599815v2\"\n          target=\"_blank\"\n          rel=\"noopener noreferrer\"\n          className=\"font-sans text-sm text-accent transition-colors hover:underline\"\n        >\n          Preprint (bioRxiv) →\n        </a>\n        <Link\n          href=\"/\"\n          className=\"font-sans text-sm text-gray-500 transition-colors hover:text-accent\"\n        >\n          ← Back to home\n        </Link>\n      </footer>\n    </article>\n  )\n}\n"],"names":[],"mappings":";;;;;;;AACA;AACA;AACA;;;;;AAEO,MAAM,WAAqB;IAChC,OAAO;IACP,aACE;AACJ;AAEe,SAAS;IACtB,qBACE,8OAAC;QAAQ,WAAU;;0BACjB,8OAAC,0LAAI;gBACH,MAAK;gBACL,WAAU;0BACX;;;;;;0BAID,8OAAC;gBAAO,WAAU;;kCAChB,8OAAC;wBAAI,WAAU;;0CACb,8OAAC,kIAAQ;gCAAC,OAAM;;;;;;0CAChB,8OAAC,kIAAQ;gCAAC,OAAM;;;;;;0CAChB,8OAAC,kIAAQ;gCAAC,OAAM;;;;;;0CAChB,8OAAC,kIAAQ;gCAAC,OAAM;;;;;;;;;;;;kCAElB,8OAAC;wBAAG,WAAU;kCAAyE;;;;;;kCAGvF,8OAAC;wBAAE,WAAU;kCAAuC;;;;;;;;;;;;0BAKtD,8OAAC;gBAAQ,WAAU;0BACjB,cAAA,8OAAC;oBAAE,WAAU;;wBAA0C;sCAG9B,8OAAC;sCAAO;;;;;;wBAAyB;sCACjC,8OAAC;sCAAO;;;;;;wBAAyB;;;;;;;;;;;;0BAY5D,8OAAC;gBAAG,WAAU;;;;;;0BAEd,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC,uIAAa;wBAAC,OAAM;;;;;;kCACrB,8OAAC;wBAAE,WAAU;kCAA+C;;;;;;kCAQ5D,8OAAC;wBAAE,WAAU;;4BAA+C;0CACtD,8OAAC;0CAAO;;;;;;4BAAyB;;;;;;;kCAQvC,8OAAC;wBAAE,WAAU;;4BAA0C;0CACjD,8OAAC;0CAAO;;;;;;4BAAyB;;;;;;;;;;;;;0BAWzC,8OAAC;gBAAG,WAAU;;;;;;0BAEd,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC,uIAAa;wBAAC,OAAM;;;;;;kCACrB,8OAAC;wBAAE,WAAU;;4BAA+C;0CAE1D,8OAAC;0CAAO;;;;;;4BAAwE;;;;;;;kCAYlF,8OAAC;wBAAE,WAAU;;0CACX,8OAAC;0CAAO;;;;;;4BAAmB;0CACE,8OAAC;0CAAO;;;;;;4BACZ;0CAEzB,8OAAC;0CAAO;;;;;;4BAA4C;0CAChB,8OAAC;0CAAO;;;;;;4BAAkC;4BAAI;;;;;;;;;;;;;0BAStF,8OAAC;gBAAG,WAAU;;;;;;0BAEd,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC,uIAAa;wBAAC,OAAM;;;;;;kCACrB,8OAAC;wBAAE,WAAU;;4BAA+C;0CAEb,8OAAC;gCAAK,WAAU;0CAA0C;;;;;;4BAAgB;4BAAK;0CAC5H,8OAAC;gCAAK,WAAU;0CAA0C;;;;;;4BAAe;0CAI3C,8OAAC;gCAAK,WAAU;0CAA0C;;;;;;4BAAc;;;;;;;kCAGxG,8OAAC;wBAAE,WAAU;;4BAA0C;0CAEf,8OAAC;gCAAK,WAAU;0CAA0C;;;;;;4BAAY;0CACjG,8OAAC;gCAAK,WAAU;0CAA0C;;;;;;4BAAgB;0CAC5E,8OAAC;gCAAK,WAAU;0CAA0C;;;;;;4BAA2B;0CAE3E,8OAAC;gCAAK,WAAU;0CAA0C;;;;;;4BAA4B;;;;;;;;;;;;;0BAK7G,8OAAC;gBAAG,WAAU;;;;;;0BAEd,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC,uIAAa;wBAAC,OAAM;;;;;;kCACrB,8OAAC;wBAAE,WAAU;kCAA+C;;;;;;;;;;;;0BAc9D,8OAAC;gBAAG,WAAU;;;;;;0BAEd,8OAAC;gBAAQ,WAAU;;kCACjB,8OAAC,uIAAa;wBAAC,OAAM;;;;;;kCACrB,8OAAC;wBAAE,WAAU;kCAA+C;;;;;;;;;;;;0BAY9D,8OAAC;gBAAO,WAAU;;kCAChB,8OAAC;wBACC,MAAK;wBACL,QAAO;wBACP,KAAI;wBACJ,WAAU;kCACX;;;;;;kCAGD,8OAAC;wBACC,MAAK;wBACL,QAAO;wBACP,KAAI;wBACJ,WAAU;kCACX;;;;;;kCAGD,8OAAC,0LAAI;wBACH,MAAK;wBACL,WAAU;kCACX;;;;;;;;;;;;;;;;;;AAMT"}}]
}